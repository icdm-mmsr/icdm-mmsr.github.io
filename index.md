---
layout: page
title: Workshop on Multimodal Search and Recommendations
subtitle: MMSR â€˜25
---

## Overview

Multimodal search and recommendation (MMSR) systems are at the forefront of modern information retrieval, designed to integrate and process diverse data types such as text, images, audio, and video within a unified framework. This integration enables more accurate and contextually relevant search results and recommendations, significantly enhancing user experiences. For example, e-commerce platforms have started supporting product searches through images to provide a streamlined shopping experience. Recent innovations in LLMs have extended their capabilities to handle multimodal inputs, allowing for a deeper and more nuanced understanding of content and user preferences.

MMSR will be a **half-day** workshop at [ICDM 2025](https://www3.cs.stonybrook.edu/~icdm2025/index.html). The workshop will be held on **November 12, 2025** in **Washington DC, USA**. The workshop will explore the latest advancements, challenges, and applications of multimodal search and recommendations.

## Important Dates

{: .box-note}
All deadlines are at 23: 59 P.M. [GMT](https://www.worldtimeserver.com/time-zones/gmt/)

| Task                                                                                             | Deadline               |
| ------------------------------------------------------------------------------------------------ | ---------------------- |
| **[Paper submission deadline](https://wi-lab.com/cyberchair/2025/icdm25/scripts/submit.php?subarea=S25&undisplay_detail=1&wh=/cyberchair/2025/icdm25/scripts/ws_submit.php)** | **September 1, 2025**    |
| Notification of acceptance                                                                   | October 1, 2025    |
| Camera Ready Version of Papers Due                       | October 12, 2025 |
| MMSR '25 Workshop                                                                       | November 12, 2025   |

## Call for Papers

Topics of interest include, but are not limited to:

1. **From Data to Discovery: Using Multimodal Models for Smarter Search and Recommendations (2025 Special Theme)**
   1. Strategies for building scalable multimodal discovery engines.
   2. Lessons learned from productionizing MMSR models in real-world applications.
   3. Handling discovery in cold-start scenarios and sparse multimodal data settings.
   4. Balancing discovery and relevance in multimodal recommendation systems.
   5. Evaluating business impact and user satisfaction of multimodal discovery systems.
   6. Emerging trends in using LLMs for multimodal data exploration and discovery.
   7. Personalization strategies tailored to multimodal discovery journeys.
   8. Bridging research and practical deployment: overcoming challenges in scaling multimodal models for search and recommendation.
2. **Cross-modal retrieval techniques**
   1. Efficiently indexing and retrieving multimodal data.
   2. Handling large-scale cross-modal data.
   3. Developing metrics to measure similarity across different modalities.
   4. Zero-shot and few-shot retrieval across unseen modalities.
   5. Adapting retrieval architectures (e.g., dual encoders vs. fusion models) for different multimodal tasks.
3. **Applications of MMSR to Verticals** (e.g., E-commerce, Healthcare, Real Estate)
   1. MMSR for image-based product search in e-commerce.
   2. Multimodal conversational agents for healthcare, legal, and retail industries.
   3. Augmented reality (AR) and multimodal discovery for shopping experiences.
   4. Customer service optimization through multimodal search interfaces (e.g., support chat, help centers).
   5. Personalized multimodal travel planning and recommendation systems.
   6. Video+text based multimodal recommendations in media and entertainment domains.
4. **User-centric design principles for MMSR interfaces**
   1. Designing user-friendly interfaces that support multimodal search.
   2. Methods for evaluating the usability of MMSR systems.
   3. Ensuring MMSR interfaces are accessible to users with disabilities.
   4. Visualizations and interactive feedback mechanisms for multimodal search refinement.
   5. A/B testing strategies specific to multimodal search UI/UX improvements.
5. **Ethical and Privacy Considerations of MMSR**
   1. Identifying and mitigating biases in multimodal algorithms.
   2. Ensuring transparency in how multimodal results are generated and presented.
   3. Approaches for obtaining and managing user consent for using user data.
   4. User perception studies of trust and explainability in multimodal search systems.
   5. Privacy-preserving multimodal modeling: federated learning and differential privacy for MMSR.
6. **Modeling for MMSR**
   1. Multi-modal representation learning.
   2. Utilizing pre-trained multimodal LLMs.
   3. Dimensionality reduction techniques to manage multimodal complexity.
   4. Fine-tuning pre-trained vision-language models.
   5. Developing and standardizing metrics to evaluate the performance of MMSR models.
   6. Alignment challenges in multimodal embeddings across diverse modalities.

#### Submission Instructions

Papers must be formatted and written according to the Submission Guidelines on the [ICDM 2025 conference web site](https://www3.cs.stonybrook.edu/~icdm2025/). You are strongly encouraged to print and double check your PDF file before its submission, especially if your paper contains Asian/European language symbols (such as Chinese/Korean characters or English letters with European fonts).

- Please note that at least **one** author of each accepted paper **must** register for the workshop.

- All accepted workshop papers will be published in the dedicated ICDMW proceedings published by the IEEE Computer Society Press.

- Non-archival submissions are not allowed, i.e., all accepted papers will be included and published in the proceedings.


## Organizers

<div style="margin-top: 2rem;display: flex; flex-wrap: wrap; gap: 1rem;">
  <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
    <img src="assets/img/aditya.jpg" alt="Aditya Chichani" style="border-radius: 50%; width: 100%; max-width: 150px; height: auto; margin-bottom: 1rem;">
    <strong><a href="https://www.linkedin.com/in/aditya-chichani/" style="text-decoration: none;color: black;">Aditya Chichani</a></strong>
  </div>
  
  <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
    <img src="assets/img/surya.png" alt="Surya Kallumadi" style="border-radius: 50%; width: 100%; max-width: 150px; height: auto; margin-bottom: 1rem;">
    <strong><a href="https://www.linkedin.com/in/surya-kallumadi-a0778a13/" style="text-decoration: none;color: black;">Surya Kallumadi</a></strong>
  </div>

  <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
    <img src="assets/img/tracy.jpeg" alt="Tracy Holloway King" style="border-radius: 50%; width: 100%; max-width: 150px; height: auto; margin-bottom: 1rem;">
    <strong>
    <a href="https://www.linkedin.com/in/tracyhollowayking/" style="text-decoration: none;color: black;">Tracy Holloway King</a>
    </strong>
  </div>

  <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
    <img src="assets/img/yubin.jpeg" alt="Yubin Kim" style="border-radius: 50%; width: 100%; max-width: 150px; height: auto; margin-bottom: 1rem;">
    <strong>
    <a href="https://www.linkedin.com/in/yubink" style="text-decoration: none;color: black;">Yubin Kim</a>
    </strong>
  </div>

  <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
    <img src="assets/img/andrei.jpeg" alt="Andrei Lopatenko" style="border-radius: 50%; width: 100%; max-width: 150px; height: auto; margin-bottom: 1rem;">
    <strong><a href="https://www.linkedin.com/in/lopatenko/" style="text-decoration: none;color: black;">Andrei Lopatenko</a></strong>
  </div>
</div>